# E2E AI Conversation API - Quick Start Guide

## ğŸ¯ What This Implements

A complete, production-ready AI conversation API endpoint with:
1. **JWT Authentication** - Secure user verification
2. **Mapped Model SSOT + Whitelist** - Client must use `/api/v1/llm/models` returned `name` as `model`
3. **Conditional Persistence** - `metadata.save_history` controls Supabase persistence (local admin token may skip)
4. **E2E Monitoring** - Real-time metrics on dashboard

## ğŸš€ Quick Test (After Implementation)

### 1. Start Services

Option Aï¼ˆæœ¬åœ°å¼€å‘ä¸€é”®å¯åŠ¨ï¼‰ï¼š
```bash
.\start-dev.ps1
```

Option Bï¼ˆDockerï¼Œæ¨èç”¨äºâ€œç¯å¢ƒä¸€è‡´æ€§â€éªŒè¯ï¼‰ï¼š
```bash
docker compose up -d --build app
```

### 2. Get JWT Token
```bash
# Local dashboard admin login (default admin/123456)
curl -X POST http://localhost:9999/api/v1/base/access_token \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "123456"}'
```

### 3. Get Model Whitelist (SSOT)
```bash
curl -s "http://localhost:9999/api/v1/llm/models?view=mapped" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"
```

Pick `data[].name` (example: `global:xai`) and use it as the `model` field.

### 4. Send AI Conversation Request

**With Model Selection + Save History**:
```bash
curl -X POST http://localhost:9999/api/v1/messages \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "What is the best workout for beginners?",
    "model": "global:xai",
    "metadata": {
      "save_history": true
    }
  }'
```

**Response**:
```json
{
  "message_id": "abc123def456",
  "conversation_id": "uuid-string"
}
```

### 5. Stream AI Response (SSE)
```bash
curl -N http://localhost:9999/api/v1/messages/abc123def456/events?conversation_id=uuid-string \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"
```

**SSE Stream**:
```
event: status
data: {"state": "queued", "message_id": "abc123def456"}

event: status
data: {"state": "working", "message_id": "abc123def456"}

event: status
data: {"state":"routed","message_id":"abc123def456","request_id":"...","provider":"xai","resolved_model":"grok-4-1-fast-reasoning","endpoint_id":28,"upstream_request_id":"..."}

event: content_delta
data: {"message_id": "abc123def456", "delta": "For beginners, I recommend..."}

event: completed
data: {"message_id":"abc123def456","reply":"...","request_id":"...","provider":"xai","resolved_model":"...","endpoint_id":28,"upstream_request_id":"..."}
```

### 6. Check Metrics
```bash
curl http://localhost:9999/api/v1/metrics | grep ai_conversation
```

**Expected Output**:
```
# HELP ai_conversation_latency_seconds AI conversation end-to-end latency
# TYPE ai_conversation_latency_seconds histogram
ai_conversation_latency_seconds_bucket{le="0.5",model="gpt-4o-mini",status="success",user_type="permanent"} 0
ai_conversation_latency_seconds_bucket{le="1.0",model="gpt-4o-mini",status="success",user_type="permanent"} 5
ai_conversation_latency_seconds_bucket{le="2.0",model="gpt-4o-mini",status="success",user_type="permanent"} 12
ai_conversation_latency_seconds_count{model="gpt-4o-mini",status="success",user_type="permanent"} 15
ai_conversation_latency_seconds_sum{model="gpt-4o-mini",status="success",user_type="permanent"} 18.5
```

### 7. View Dashboard
Open http://localhost:3101/dashboard

**Expected**:
- "AI å¯¹è¯ç«¯ç‚¹ç›‘æ§" card showing:
  - P50 å»¶è¿Ÿ: ~1200ms
  - P95 å»¶è¿Ÿ: ~1800ms
  - P99 å»¶è¿Ÿ: ~2500ms
  - æˆåŠŸç‡: 100%
  - æ€»è¯·æ±‚æ•°: 15
  - æ´»è·ƒå¯¹è¯: 2

---

## ğŸ“‹ Implementation Checklist

### Phase 1: Mapped Model SSOT + Whitelist âœ…
- [x] `/api/v1/llm/models` è¿”å›æ˜ å°„ç™½åå•ï¼ˆ`name` å³å®¢æˆ·ç«¯å¯å‘é€ `model`ï¼‰
- [x] `POST /api/v1/messages` å¼ºæ ¡éªŒ `model` å¿…é¡»åœ¨ç™½åå•ï¼ˆ422 å« request_idï¼‰

### Phase 2: Conditional Persistence âœ…
- [x] `metadata.save_history=false` è·³è¿‡ Supabase è½åº“
- [x] local admin/test tokenï¼ˆé UUID subï¼‰è‡ªåŠ¨è·³è¿‡ Supabase è½åº“ï¼ˆé¿å…å™ªå£°ï¼‰

### Phase 3: Prometheus Metrics âœ…
- [ ] Add `ai_conversation_latency` histogram to `metrics.py`
- [ ] Record latency in `run_conversation()` finally block
- [ ] Test: `/api/v1/metrics` shows histogram
- [ ] Test: Buckets contain counts
- [ ] Test: Labels include model, user_type, status

### Phase 4: Frontend Display âœ…
- [ ] Create `AiConversationMetrics.vue` component
- [ ] Implement percentile calculation from histogram
- [ ] Add to dashboard layout
- [ ] Test: Metrics display correctly
- [ ] Test: Auto-refresh works
- [ ] Test: Model breakdown shows

---

## ğŸ§ª Test Scenarios

### Scenario 1: Model Selection
```bash
# Get whitelist (SSOT)
GET /api/v1/llm/models

# Use returned data[].name as model
POST /api/v1/messages
{
  "text": "Hello",
  "model": "global:xai"
}
# Expected: routes to provider+endpoint and emits status:routed for audit
```

### Scenario 2: Conditional Persistence
```bash
# Test save enabled
POST /api/v1/messages
{
  "text": "Hello",
  "metadata": {"save_history": true}
}
# Expected: Record in Supabase `public.conversations` + `public.messages`ï¼ˆB2 SSOTï¼‰

# Test save disabled
POST /api/v1/messages
{
  "text": "Hello",
  "metadata": {"save_history": false}
}
# Expected: NO record in Supabase

# Test default behavior
POST /api/v1/messages
{
  "text": "Hello"
}
# Expected: Record in Supabaseï¼ˆB2 SSOTï¼Œbackward compatibleï¼‰
```

### Scenario 3: Error Handling
```bash
# Test invalid JWT
POST /api/v1/messages (no Authorization header)
# Expected: 401 Unauthorized

# Test invalid model (whitelist enforced)
POST /api/v1/messages
{
  "text": "Hello",
  "model": "invalid-model-xyz"
}
# Expected: 422 model_not_allowed (+ request_id)

# Test empty text
POST /api/v1/messages
{
  "text": ""
}
# Expected: 422 Validation Error
```

---

## ğŸ“Š Monitoring Queries

### Prometheus Queries (for Grafana)

**P50 Latency**:
```promql
histogram_quantile(0.50, 
  rate(ai_conversation_latency_seconds_bucket[5m])
)
```

**P95 Latency**:
```promql
histogram_quantile(0.95, 
  rate(ai_conversation_latency_seconds_bucket[5m])
)
```

**Success Rate**:
```promql
sum(rate(ai_conversation_latency_seconds_count{status="success"}[5m])) 
/ 
sum(rate(ai_conversation_latency_seconds_count[5m])) * 100
```

**Requests per Model**:
```promql
sum by (model) (
  rate(ai_conversation_latency_seconds_count[5m])
)
```

---

## ğŸ”§ Troubleshooting

### Issue: Metrics not showing
**Solution**: 
1. Check `/api/v1/metrics` endpoint
2. Verify histogram name: `ai_conversation_latency_seconds`
3. Check Prometheus scrape config

### Issue: Supabase save failing
**Solution**:
1. Check `SUPABASE_PROJECT_ID` and `SUPABASE_SERVICE_ROLE_KEY` in `.env`
2. Verify `public.conversations` / `public.messages` table exists
3. Check logs for `sync_chat_record` errors

### Issue: Model not being used
**Solution**:
1. Verify `model` uses `/api/v1/llm/models` returned `data[].name`
2. Verify endpoint has api_key and is not offline
3. Check SSE `status:routed` for `provider/resolved_model/endpoint_id`

### Issue: tool_calls_not_supported / upstream_empty_content
**Solution**:
1. å½“å‰åç«¯ä¸æ‰§è¡Œå·¥å…·ï¼šserver æ¨¡å¼é»˜è®¤ `tool_choice=none`
2. é€ä¼ æ¨¡å¼è‹¥æä¾› toolsï¼Œå»ºè®® `tool_choice=none`ï¼Œæˆ–å®ç°å·¥å…·æ‰§è¡Œå™¨åå†å¼€å¯ `auto`

---

## âœ… Real xAI E2E (Recommended)

åœ¨ä»“åº“æ ¹ç›®å½• `.env.local` é…ç½®ï¼ˆä¸è¦æäº¤åˆ° gitï¼‰ï¼š
```bash
XAI_BASE_URL=https://api.x.ai
XAI_API_KEY=<redacted>
```

è¿è¡Œï¼ˆä¼šè‡ªåŠ¨æŠŠ `grok-4.1-æ€è€ƒ` è§£æä¸º xAI å®é™…å¯ç”¨ model id å¹¶å®Œæˆ server/passthrough ä¸¤ç§æ¨¡å¼ï¼‰ï¼š
```bash
.venv/bin/python scripts/monitoring/xai_mapped_model_passthrough_e2e.py
```

Docker ä¸‹è¿è¡Œï¼ˆæœåŠ¡å·²æ˜ å°„åˆ°å®¿ä¸» `9999`ï¼‰ï¼š
```bash
E2E_API_BASE=http://127.0.0.1:9999/api/v1 .venv/bin/python scripts/monitoring/xai_mapped_model_passthrough_e2e.py
```

### Issue: Dashboard not updating
**Solution**:
1. Check browser console for errors
2. Verify `/api/v1/metrics` returns data
3. Check auto-refresh interval setting

---

## ğŸ“š API Reference

### POST /api/v1/messages

**Headers**:
- `Authorization: Bearer <JWT_TOKEN>` (required)
- `Content-Type: application/json`

**Request Body**:
```typescript
{
  // At least one of: text / messages
  text?: string;             // User message
  messages?: any[];          // OpenAI messages
  conversation_id?: string;  // Optional: Conversation ID
  model: string;             // Required: use `/api/v1/llm/models?view=mapped` returned `data[].name`
  skip_prompt?: boolean;     // Optional: passthrough mode
  system_prompt?: string;    // Optional
  tools?: any[];             // Optional
  tool_choice?: any;         // Optional
  metadata?: {
    save_history?: boolean;  // Optional: Save to Supabase (default: true)
    [key: string]: any;      // Other metadata
  };
}
```

**Response** (202 Accepted):
```json
{
  "message_id": "abc123def456",
  "conversation_id": "uuid-string"
}
```

**Errors**:
- `401 Unauthorized`: Invalid/missing JWT
- `422 Validation Error`: Invalid request body
- `429 Too Many Requests`: Rate limit exceeded
- `500 Internal Server Error`: AI provider error

### GET /api/v1/messages/{message_id}/events

**Headers**:
- `Authorization: Bearer <JWT_TOKEN>` (required)

**Query Parameters**:
- `conversation_id` (optional): Conversation ID for concurrency control

**Response** (SSE Stream):
```
event: status
data: {"state": "queued", "message_id": "..."}

event: status
data: {"state": "working", "message_id": "..."}

event: content_delta
data: {"message_id": "...", "delta": "chunk of text"}

event: completed
data: {"message_id": "...", "reply": "full response"}

event: error
data: {"message_id": "...", "error": "error message"}
```

---

## ğŸ“ Next Steps

1. **Implement Phase 1-4** following the implementation plan
2. **Run tests** to verify each phase
3. **Deploy to staging** for integration testing
4. **Update mobile app** to send `model` and `save_history` fields
5. **Monitor dashboard** for real-time metrics
6. **Set up alerts** for latency/error rate thresholds

---

## ğŸ“ Support

- **Documentation**: `docs/e2e-ai-conversation/IMPLEMENTATION_PLAN.md`
- **Architecture**: `docs/dashboard-refactor/ARCHITECTURE_OVERVIEW.md`
- **JWT Guide**: `docs/JWT_HARDENING_GUIDE.md`
- **API Monitoring**: `docs/API_MONITOR_HANDOVER.md`
